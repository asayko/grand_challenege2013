% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{amsmath}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{Report on a baseline approach to the second MSR-Bing Challenge on Image Retrieval.}
\author{Aleksandr Sayko, Anton Slesarev}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\abstract{In the report we present a baseline approach for solving the problem which was set within the second MSR-Bing Challenge on Image Retrieval \cite{GrandChallengeSite}. Our goal is to present a simple method to calculate image's relevance to the given search query based on similarity matching with pictures from click log. The relevance is estimated in two stages. Within the first stage we fetch from the click log images that  were clicked on alike search queries. Within the second stage we calculate the cumulative similarity of given image to the fetched pictures. The cumulative similarity is then returned as requested image relevance.}

\section{Extracting click-relevant images from the  click log for a query.}

Our purpose at the first stage is to mine from the click log pictures that we could possibly use as relevant examples to the given search query. In order to find such images we build four mappings. The first index maps normalized search queries from the click log to the images that were clicked on the normalized query. We also save total number of clicks in the index to be able to take top most clicked on the query pictures. For normalization we lematize the words of the query delete stop words and sort the rest lexicographically. We exploit lematization from the NLTK framework \cite{NLTKSite}.

The second index maps query lemmas to pictures that were clicked on the queries containing the lemmas. In the index we also save the cumulative number of clicks and the number of different queries on which the picture was clicked. The numbers are needed to fetch top relevant images for a lemma, where relevance is estimated using heuristic rules.

The third and the fourth indicies are alike with the first one but they store bigramms and respectively trigramms that are found in the click log. By ngramm we mean sorted tuple of lemmas contained in the search query.

\subsection{Search query processing.}

The search query is processed the following way. At first we normalize the query. For each query lemma we generate a set of synsets\cite{NLTKSite}. Then we create all possible bigramms and trigramms building all combinations of elements from different sets. After that we try to retrieve 100 images from the four indecies into "top 100 related pics" list. At first we retrieve top 100 most clicked images from index build upon normalized queries. If there are less than 100 clicked images we try to fetch lacking pictures from other indecies using following heuristical rule. For each generated from search query ngramm fetch all clicked images from the ngramm indecies. For each  of the extracted images estimate the click relevance. The estimated click relevance of a picture is the sum over all extraction cases(that are caused by machted ngramms) of products of the number of different queries containing the ngramm the image was clicked on with the logarithm of the total number of clicks the picture got on queries containing the ngramm and number of noun-like lemmas in the ngramm and the ngramm match type coeficient, where ngramm match type coeficients are empirically tuned. Sort the retrieved images by decrease of estimated click relevance. Try  to complete the "top 100 related pics" list up to 100 images with the top images from the retrieved list.

The proposed heuristical approach enables to fetch visually consistent lists of  pictures for considerable part of search queries from the provided test set, which could be possibly used for training query specific classifyers.  As a proof of the concept we used the list as relevant examples and estimated requested relevance of the given picture as sum of visual similarities with retrieved images.

\section{Comparing given image with click-relevant images for the query.}
To calculate visual similarity score between two images we use a standard bag-of-words
framework \cite{Sivic03}, with images represented as L2 normalized histograms of
visual words. We use rather small vocabulary consisted of 32768 visual
words, trained from SIFT\cite{Lowe:2004} descriptors. If an L2 distance between
histograms less than a threshold we believe that two images have
something in common. After that we consider features as matches if
features assign to the same visual vocabulary cluster. We apply
RANSAC-based geometric verification to find a maximal set of feature
matches such that features in one image can be mapped to their
corresponding features in the other by a similarity transformation.
We also tried more complicated transformations but did not receive any
profit. Matches which correspond to the best transformations are
called inliers. The number of inliers characterizes similarity of two images.

We can describe the final score S as follows:

$$
    S = 
\begin{cases}
    0,& \text{if L2 hist dist} > thr\\
    I / \sqrt{N_1, N_2},              & \text{otherwise}
\end{cases}
$$
where I is the number of inliers, $N_1$ and $N_2$ are the number of
descriptors on the first and the second image consequently.

We use square root normalization because some works\cite{jegou:inria-00602325} showed that it
outperforms standard normalization on the number of descriptors.


\bibliographystyle{abbrv}
\bibliography{bib.bib}

\end{document}
